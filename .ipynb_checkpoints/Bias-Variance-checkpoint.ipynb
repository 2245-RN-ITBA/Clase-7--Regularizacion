{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuál modelo es mejor?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"three_models.png\" alt=\"Drawing\" style=\"width:60%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué comportamiento tienen un modelo de alto bias vs un modelo de alta varianza en train? ¿Y en test?\n",
    "\n",
    "Entre un pantalón que me queda chico y uno que me queda grande, ¿cuál elijo? \n",
    "\n",
    "Elegimos el que queda grande y buscamos soluciones, como un cinturón.\n",
    "\n",
    "Vamos a buscar sobreajustar el modelo para luego usar distintas técnicas que nos permiten controlar el overfitting.\n",
    "\n",
    "Intuición: ¿Cuándo hay más overfitting?\n",
    "\n",
    "Pocas observaciones - Muchas observaciones\n",
    "\n",
    "Pocos parámetros - Muchos parámetros \n",
    "\n",
    "Pocas epochs - Muchas epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"masteryoda.jpeg\" alt=\"Drawing\" style=\"width:70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cantidad de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "from IPython.display import HTML\n",
    "import reg_helper as RHelper\n",
    "import draw_nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = np.load('X_train.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_train = np.load('y_train.npy') \n",
    "y_test = np.load('y_test.npy')\n",
    "print('Cantidad de observaciones:')\n",
    "print('Train',X_train.shape[0])\n",
    "print('Test',X_test.shape[0])\n",
    "print('Dos variables de entrada. Dos clases de salida (binario)')\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "RHelper.plot_boundaries(X_train, y_train, ax=ax1)\n",
    "RHelper.plot_boundaries(X_test, y_test, ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regresión logística polinomial**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El logit es una ecuación polinómica de las dos entradas. La cantidad de parámetros a ajustar depende del grado del polinomio.\n",
    "\n",
    "\\begin{equation}\n",
    "\\large\n",
    "a = w_0 + x_1w_1 + x_2w_2 + x_1x_2w_3 + w_4x_1^2 + w_5x_2^2 + ... + w_Nx_1^K\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\large\n",
    "y = \\sigma(a)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Ajustemos una recta (polinomio de grado 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_acc, ts_acc, coefs = RHelper.fit_and_get_regions(X_train, y_train, X_test, y_test, degree=1)\n",
    "print('Acurracy en train', tr_acc)\n",
    "print('Accuracy en test', ts_acc)\n",
    "print('Cantidad de parámetros', coefs.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos una cuadrática..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_acc, ts_acc, coefs = RHelper.fit_and_get_regions(X_train, y_train, X_test, y_test, degree=2)\n",
    "print('Acurracy en train', tr_acc)\n",
    "print('Accuracy en test', ts_acc)\n",
    "print('Cantidad de parámetros', coefs.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_acc, ts_acc, coefs = RHelper.fit_and_get_regions(X_train, y_train, X_test, y_test, degree=6)\n",
    "print('Acurracy en train', tr_acc)\n",
    "print('Accuracy en test', ts_acc)\n",
    "print('Cantidad de parámetros', coefs.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_acc, ts_acc, coefs = RHelper.fit_and_get_regions(X_train, y_train, X_test, y_test, degree=18)\n",
    "print('Acurracy en train', tr_acc)\n",
    "print('Accuracy en test', ts_acc)\n",
    "print('Cantidad de parámetros', coefs.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿En qué modelo empezamos a sobreajustar? ¿Qué harían para elegir el mejor modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [{'degree': 1, 'lambd': 0}, \n",
    "           {'degree': 2, 'lambd': 0}, \n",
    "           {'degree': 3, 'lambd': 0}, \n",
    "           {'degree': 4, 'lambd': 0}, \n",
    "           {'degree': 5, 'lambd': 0}, \n",
    "           {'degree': 6, 'lambd': 0}, \n",
    "           {'degree': 7, 'lambd': 0}, \n",
    "           {'degree': 8, 'lambd': 0}, \n",
    "           {'degree': 9, 'lambd': 0}, \n",
    "           {'degree': 10, 'lambd': 0},\n",
    "           {'degree': 11, 'lambd': 0},\n",
    "           {'degree': 12, 'lambd': 0},\n",
    "           {'degree': 13, 'lambd': 0},\n",
    "           {'degree': 14, 'lambd': 0}, \n",
    "           {'degree': 15, 'lambd': 0}, \n",
    "           {'degree': 16, 'lambd': 0}, \n",
    "           {'degree': 17, 'lambd': 0}, \n",
    "           {'degree': 18, 'lambd': 0}\n",
    "           ]\n",
    "degrees, lambdas, train_acc_array, test_acc_array, coefs_array_mean, coefs_array_std, coefs_abs_max, coefs_norm, coefs_num = RHelper.test_options(X_train, y_train, X_test, y_test, options, plot_it=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "ax.plot(degrees, train_acc_array, label=\"Train\")\n",
    "ax.plot(degrees, test_acc_array, label=\"Test\")\n",
    "plt.title(\"Accuracies\")\n",
    "\n",
    "plt.xlabel('Orden del polinomio')\n",
    "plt.ylabel('Accuracy')\n",
    "ax.set_xticks(degrees)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,5))\n",
    "ax.plot(degrees, coefs_num)\n",
    "ax.legend()\n",
    "plt.xlabel('Orden del polinomio')\n",
    "plt.ylabel('Cantidad de parámetros')\n",
    "ax.set_xticks(degrees)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.ylabel(\"Norma de vector de pesos\")\n",
    "plt.plot(degrees, coefs_norm)\n",
    "plt.xlabel('Orden del polinomio')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos la función de costo de entropía cruzada para una observación:\n",
    "\n",
    "$Loss = - \\sum_i [p_i  \\log(\\hat{p}_i) + (1-p_i) \\log(1-\\hat{p}_i)]$  \n",
    "\n",
    "donde: \n",
    "- $p_i$ solo puede valer 1 o 0. Vale 1 si pertenece a la clase $i$'esima, y 0 si no pertenece\n",
    "- $\\hat{p}_i$ es la estimación de la probabilidad de que $X_i$ pertenezca a la clase. Por ejemplo, la sigmoidea de la ecuación polinómica de pesos\n",
    "\n",
    "La regularización busca conseguir modelos más generalizables, es decir, con menor sobreajuste. Esto se logra mediante modelos con pesos bajos, que se obtienen penalizando los pesos altos. Para ello, se agrega un término a la función de costo que es función del módulo de los pesos.\n",
    "\n",
    "$Loss = - \\sum_i [p_i  \\log(\\hat{p}_i) + (1-p_i) \\log(1-\\hat{p}_i)] + f(\\mathbf {w})$  \n",
    "\n",
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LASSO**: normalización de norma L1\n",
    "\n",
    "$Loss = - \\sum_i [p_i  \\log(\\hat{p}_i) + (1-p_i) \\log(1-\\hat{p}_i)] + \\lambda  ||\\mathbf {w}||_1$ \n",
    "\n",
    "$||\\mathbf {w}||_1 = |w_1|+|w_2|+...+|w_n|$\n",
    "\n",
    "<img src=\"lasso_penalty.png\" alt=\"Drawing\" style=\"width:30%;\"/>\n",
    "\n",
    "La disminución en el valor de loss depende de cuánto cambie el valor de un peso, pero no depende del valor del peso. \n",
    "\n",
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RIDGE**: normalización de norma L2\n",
    "\n",
    "$Loss = - \\sum_i [p_i  \\log(\\hat{p}_i) + (1-p_i) \\log(1-\\hat{p}_i)] + \\lambda   ||\\mathbf {w}||_2$ \n",
    "\n",
    "$||\\mathbf {w}||_2 = \\sqrt{w_1^2+w_2^2+...+w_n^2}$\n",
    "\n",
    "<img src=\"ridge_penalty.png\" alt=\"Drawing\" style=\"width:30%;\"/>\n",
    "\n",
    "\n",
    "La disminución en el valor de loss será distinta según el valor del peso "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica con Pesos y Alturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/rn-2019-itba/Clase-4---LDA---QDA---RL---DT---RF/master/data/alturas-pesos-mils-train.csv')\n",
    "data = df[['Altura', 'Peso']].values\n",
    "y=np.array([0 if x=='Hombre' else 1 for x in df['Genero']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,y, test_size=0.4, shuffle=True, random_state=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_sin_regu = LogisticRegression()\n",
    "clf_sin_regu.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.scatter(data[y==0,0],data[y==0,1], marker='+')\n",
    "plt.scatter(data[y==1,0],data[y==1,1], c= 'green', marker='o')\n",
    "w = clf_sin_regu.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(min(data[:,0]),max(data[:,0]), 100)\n",
    "yy = a * xx - (clf_sin_regu.intercept_[0]) / w[1]\n",
    "\n",
    "plt.plot(xx, yy, 'k-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLinearBoundary(X,y,clf):\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    plt.scatter(X[y==0,0],X[y==0,1], marker='+')\n",
    "    plt.scatter(X[y==1,0],X[y==1,1], c= 'green', marker='o')\n",
    "    w = np.squeeze(clf.coef_)\n",
    "    a = -w[0] / w[1]\n",
    "    x0 = np.linspace(min(X[:,0]),max(X[:,0]), 100)\n",
    "    x1 = a * x0 - (np.squeeze(clf.intercept_)) / w[1]\n",
    "\n",
    "    plt.plot(x0, x1, 'k-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lasso = Lasso(alpha=0.1)\n",
    "clf_lasso.fit(X_train,y_train)\n",
    "plotLinearBoundary(data,y,clf_lasso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ridge = Ridge(alpha=0.1)\n",
    "clf_ridge.fit(X_train,y_train)\n",
    "\n",
    "plotLinearBoundary(data, y, clf_ridge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBoundary(data, labels, clf_1, N=300,degree=False,include_bias=True):\n",
    "    class_1 = data[labels == 1]\n",
    "    class_0 = data[labels == 0]\n",
    "    mins = data[:,:2].min(axis=0)\n",
    "    maxs = data[:,:2].max(axis=0)\n",
    "    x1 = np.linspace(mins[0], maxs[0], N)\n",
    "    x2 = np.linspace(mins[1], maxs[1], N)\n",
    "    x1, x2 = np.meshgrid(x1, x2)\n",
    "    X=np.c_[x1.flatten(), x2.flatten()]\n",
    "    if degree is not None:\n",
    "        poly=PolynomialFeatures(degree,include_bias=include_bias)\n",
    "        X=poly.fit_transform(X)\n",
    "    Z_nn = clf_1.predict(X)#[:, 0]\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z_nn = Z_nn.reshape(x1.shape)\n",
    "    \n",
    "    fig = plt.figure(5,figsize=(15,15))\n",
    "    ax = fig.gca()\n",
    "    cm = plt.cm.RdBu\n",
    "        \n",
    "    ax.contour(x1, x2, Z_nn, (0.5,), colors='black', linewidths=1)\n",
    "    ax.scatter(class_1[:,0], class_1[:,1], c= 'green', marker='o', s=20, alpha=0.5)\n",
    "    ax.scatter(class_0[:,0], class_0[:,1], c= 'blue', marker='+', s=20, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "order = 4\n",
    "poly = PolynomialFeatures(order,include_bias=True)\n",
    "X_poly_train =poly.fit_transform(X_train)\n",
    "lasso_poly = Lasso(alpha=0.8)\n",
    "lasso_poly.fit(X_poly_train,y_train)\n",
    "print(\"Cantidad de features seleccionadas: \", np.sum(lasso_poly.coef_!=0), \" de \", len(lasso_poly.coef_))\n",
    "plotBoundary(data, y, lasso_poly, 1000,degree=order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_poly = Ridge(alpha=0.8)\n",
    "ridge_poly.fit(X_poly_train,y_train)\n",
    "print(\"Cantidad de features seleccionadas: \", np.sum(ridge_poly.coef_!=0), \" de \", len(ridge_poly.coef_))\n",
    "plotBoundary(data, y, ridge_poly, 1000,degree=order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Red neuronal multicapa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 2\n",
    "hidden_units = 20 \n",
    "output_size = 1\n",
    "network = draw_nn.DrawNN( [input_shape, hidden_units, output_size] )\n",
    "network.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def get_two_layer_model_compiled(input_shape, output_size, hidden_units, lr=2,decay=0.0):\n",
    "\n",
    "    model = Sequential()\n",
    "    sgd = optimizers.SGD(lr=lr, decay=decay)\n",
    "    model.add(Dense(hidden_units,input_dim=input_shape,  activation='sigmoid', ))\n",
    "    model.add(Dense(output_size, \n",
    "                    activation='sigmoid', \n",
    "                    kernel_initializer='zeros', \n",
    "                    name='Salida'\n",
    "                   ))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnn_helper import PlotLosses\n",
    "from keras import optimizers\n",
    "\n",
    "lr=1\n",
    "decay=0\n",
    "\n",
    "plot_losses = PlotLosses(plot_interval=200, evaluate_interval=None, x_val=X_test, y_val_categorical=y_test)\n",
    "two_layer_model = get_two_layer_model_compiled(input_shape, \n",
    "                                                 output_size, \n",
    "                                                 hidden_units, lr, decay \n",
    "                                                )\n",
    "two_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs = 10\n",
    "\n",
    "hist = two_layer_model.fit(X_train, \n",
    "          y_train,\n",
    "          batch_size=5,\n",
    "          epochs=epochs, \n",
    "          verbose=1, \n",
    "          validation_data=(X_test, y_test), \n",
    "          #callbacks=[plot_losses],\n",
    "         )\n",
    "\n",
    "print('Cantidad de parámetros',two_layer_model.count_params())\n",
    "print('Mejor acc en train', max(hist.history['acc']))\n",
    "print('Mejor acc en test', max(hist.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 2 \n",
    "two_layer_model = get_two_layer_model_compiled(input_shape, \n",
    "                                                 output_size, \n",
    "                                                 hidden_units=hidden_units, \n",
    "                                                 lr=lr, \n",
    "                                                 decay=decay,\n",
    "                                                )\n",
    "two_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = two_layer_model.fit(X_train, \n",
    "          y_train, batch_size = batch_size,\n",
    "          epochs=epochs, \n",
    "          verbose=0, \n",
    "          validation_data=(X_test, y_test), \n",
    "          #callbacks=[plot_losses],\n",
    "         )\n",
    "print('Cantidad de parámetros',two_layer_model.count_params())\n",
    "print('Mejor acc en train', max(hist.history['acc']))\n",
    "print('Mejor acc en test', max(hist.history['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 200 \n",
    "two_layer_model = get_two_layer_model_compiled(input_shape, \n",
    "                                                 output_size, \n",
    "                                                 hidden_units=hidden_units, \n",
    "                                                 lr=lr, \n",
    "                                                 decay=decay,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = two_layer_model.fit(X_train, \n",
    "          y_train, batch_size = batch_size,\n",
    "          epochs=epochs, \n",
    "          verbose=0, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[plot_losses],\n",
    "         )\n",
    "print('Cantidad de parámetros',two_layer_model.count_params())\n",
    "print('Mejor acc en train', max(hist.history['acc']))\n",
    "print('Mejor acc en test', max(hist.history['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tamaño del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_200 = np.load('200_samples_OK.npy')\n",
    "dataset_200 = np.load('1000_samples.npy')\n",
    "\n",
    "X = dataset_200[:,:2]\n",
    "y = dataset_200[:, 2]\n",
    "f = plt.figure(figsize=(20,6))\n",
    "RHelper.plot_boundaries(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(10)\n",
    "splited_indexs = skf.split(X, y)\n",
    "print(type(splited_indexs))\n",
    "print(next(splited_indexs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "training_sets = []\n",
    "for train_index, test_index in splited_indexs:\n",
    "    i=i+1\n",
    "    print(\"CV dataset:\", i)\n",
    "    print(train_index.shape, test_index.shape)\n",
    "    dictionary = {'X_train':X[train_index], 'y_train':y[train_index], 'X_test':X[test_index],'y_test':y[test_index]}\n",
    "    training_sets.append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "RHelper.plot_boundaries(training_sets[idx]['X_train'],training_sets[idx]['y_train'])\n",
    "plt.title('Train subset in fold ' + str(idx))\n",
    "plt.show()\n",
    "plt.title('Test subset in fold ' + str(idx))\n",
    "RHelper.plot_boundaries(training_sets[idx]['X_test'],training_sets[idx]['y_test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "classifier = KerasClassifier(build_fn=get_two_layer_model_compiled(input_shape, \n",
    "                                      output_size, \n",
    "                                      hidden_units), batch_size=25,  \n",
    "                             epochs=10, verbose=0)\n",
    "cross_val_scores = cross_val_score(estimator=classifier, X=X, y=y, cv=skf, \n",
    "                                   scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularización en redes neuronales\n",
    "**Regularizers de Keras**\n",
    "\n",
    "En una red neuronal, la regularización se define en cada capa. Puedo definir los pesos de cuáles capas se sumarán en la función de costo. Usando Keras, puedo definir el 'regularizer' de cada capa. \n",
    "Puedo asignar un regularizer como 'kernel_regularizer', 'bias_regularizer', o 'activity_regularizer'. El que hemos explicado hasta ahora, es decir, penalizando los pesos altos, es el kernel_regularizer. El bias regularizer penaliza un alto bias, y el activity_regularizer penaliza un total alto de la ecuacion $wx+b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "#Los que ya están implementados en Keras son: \n",
    "\n",
    "regularizers.l1(0.)\n",
    "regularizers.l2(0.)\n",
    "regularizers.l1_l2(l1=0.01, l2=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout**\n",
    "\n",
    "La técnica de dropout consiste en ignorar ciertas unidades de la red durante el entrenamiento. \n",
    "\n",
    "<img src=\"dropout.png\" alt=\"Drawing\" style=\"width:60%;\"/>\n",
    "\n",
    "Esto se consigue asignando un peso nulo a la conexión entre esta unidad y la siguiente capa (ignorarla en el *foward propagation*) y un peso nulo a su conexiones con las unidades de la capa anterior (ignorarla en el *backpropagation*). La decisión de si una unidad es ignorada o no en cada iteración del entrenamiento es aleatoria: se define como una probabilidad $p$. Este es el único hiperparámetro del dropout. Cada nodo individual tiene una probabilidad $p$ de ser conservado (es decir, $1-p$ de ser ignorado).\n",
    "\n",
    "Durante la etapa de testeo, se usan todos los nodos pero se reducen sus pesos en un factor $p$.\n",
    "\n",
    "El dropout en Keras se agrega como una capa de tipo Dropout, que se aplica a las conexiones entre la capa inmediatamente anterior e inmediatamente posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# Compile model\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
